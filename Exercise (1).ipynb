{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea125a4d-13da-4c00-8a8d-2f24d6425ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: /usr/share/pip-wheels\n",
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[2K     \u001b[38;5;70m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: emoji in /home/1913c543-331c-4721-95ae-6d87e099a045/.local/lib/python3.12/site-packages (2.15.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (4.14.2)\n",
      "Requirement already satisfied: pandas in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (2.3.3)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: click in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from nltk) (2025.9.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/anaconda-2025.12-py312/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622414 sha256=45e30749344ba5bde1b7fddaff403ed767b0773086e67c704977314a06e9af72\n",
      "  Stored in directory: /home/1913c543-331c-4721-95ae-6d87e099a045/.cache/pip/wheels/b6/28/c2/9ddf8f57f871b55b6fd0ab99c887531fb9a66e5ff236b82aee\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect emoji beautifulsoup4 pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02846d3a-c41f-474b-aabe-c33e907b09c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /home/1913c543-331c-4721-95ae-\n",
      "[nltk_data]     6d87e099a045/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reviews... this may take a moment due to spellcheck.\n",
      "                                                                                                                                                                                                                                                                                                                                                         Review  \\\n",
      "0                                                                                                                                                                                                                                                                                                          Im happy with uniten actually, even the people are W   \n",
      "1                                                                                                                                                                                                                                                                                        I’m having a pretty good time here, happy to meet all of the W people.   \n",
      "2                                                                                                                                                                                                                                                                                                                   a very neutral place in terms of everything   \n",
      "3                                                                                                                                                                                                   I would say Uniten it's  a good university  but there is some issue need to be improved such as transportation,wifi networks and other facilities  as well.   \n",
      "4   UNITEN is well-regarded, particularly for its strong engineering, computer science, and business programs. It has a solid reputation in Malaysia, especially in energy-related fields. The negative part is the facilities such as the swimming pool are close since my first year till 3 year now and there are limited parking so it's make hard to find.   \n",
      "\n",
      "                                                                                                                                                                                                                                    processed  \n",
      "0                                                                                                                                                                                                       im happy unite actually even people w  \n",
      "1                                                                                                                                                                                                  i ’ m pretty good time happy meet w people  \n",
      "2                                                                                                                                                                                                               neutral place term everything  \n",
      "3                                                                                                                                                would say united good university issue need improve transportationwifi network facility well  \n",
      "4  united wellregarded particularly strong engineering computer science business program solid reputation malaysia especially energyrelated field negative part facility swim pool close since first year till year limit park make hard find  \n",
      "File saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download required NLTK data (Added error handling for newer versions)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab') # Required for newer NLTK versions\n",
    "try:\n",
    "    nltk.download('averaged_perceptron_tagger_eng')\n",
    "except:\n",
    "    nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize tools\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Slang and Contractions Dictionaries\n",
    "slang_dict = {\n",
    "    \"tbh\": \"to be honest\", \"omg\": \"oh my god\", \"lol\": \"laugh out loud\",\n",
    "    \"idk\": \"i do not know\", \"brb\": \"be right back\", \"btw\": \"by the way\",\n",
    "    \"imo\": \"in my opinion\", \"smh\": \"shaking my head\", \"fyi\": \"for your information\", \"np\": \"no problem\"\n",
    "}\n",
    "\n",
    "contractions_dict = {\n",
    "    \"can't\": \"cannot\", \"won't\": \"will not\", \"don't\": \"do not\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\", \"i'm\": \"i am\",\n",
    "    \"it's\": \"it is\", \"you're\": \"you are\", \"they're\": \"they are\"\n",
    "}\n",
    "\n",
    "# Build contractions regex\n",
    "pattern = r'\\b(' + \"|\".join(re.escape(key) for key in contractions_dict.keys()) + r')\\b'\n",
    "compiled_pattern = re.compile(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "def replace_slang(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([slang_dict.get(word.lower(), word) for word in words])\n",
    "\n",
    "def replace_contractions(text):\n",
    "    return compiled_pattern.sub(lambda m: contractions_dict[m.group(0).lower()], text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    return spell(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_words])\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'): return wordnet.ADJ\n",
    "    elif tag.startswith('V'): return wordnet.VERB\n",
    "    elif tag.startswith('N'): return wordnet.NOUN\n",
    "    elif tag.startswith('R'): return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return \"\"\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "    return \" \".join([lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags])\n",
    "\n",
    "# --- Main Pipeline ---\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text): return \"\"\n",
    "    text = text.lower()\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = replace_slang(text)\n",
    "    text = replace_contractions(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = correct_spelling(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    return text\n",
    "\n",
    "# Load, Apply, and Save\n",
    "try:\n",
    "    df = pd.read_csv(\"UNITENReview.csv\")\n",
    "    print(\"Processing reviews... this may take a moment due to spellcheck.\")\n",
    "    df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "    \n",
    "    print(df[[\"Review\", \"processed\"]].head())\n",
    "    df.to_csv(\"UNITEN_Processed.csv\", index=False)\n",
    "    print(\"File saved successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'UNITENReview.csv' not found. Please ensure the file is in the same folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272a116-9da6-4709-b1d2-905ecf30c1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
